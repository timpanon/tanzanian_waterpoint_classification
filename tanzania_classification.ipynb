{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanzania Waterpoint Classification Model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook I attempt to build a supervised ternary classification model to predict the operational status of waterpoints in Tanzania. To do so, I analyze data on waterpoints and then build three machine learning models using chosen features from the dataset. Each model is evaluated and optimized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Audience/Business Problem \n",
    "Here I sought to build a model to predict waterpoint status and unlock insights that would be useful to the Tanzanian government or party interested in the maintenance/repair of waterpoints. By using a machine learning model to categorize waterpoints by operational status, time and resources could be theoretically better allocated. Waterpoints which need maintenance / repair could be prioritized without a visit to each. \n",
    "\n",
    "Objective:\n",
    "Build a supervised classification model which can predict the operational status of a waterpoint belonging to one of three categories:\n",
    "* Functional \n",
    "* Non-functional\n",
    "* Functional but needing repairs \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import folium\n",
    "import math\n",
    "\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, log_loss,\\\n",
    "accuracy_score, confusion_matrix, plot_confusion_matrix, make_scorer, mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Links to Notebook Sections\n",
    "* [Preprocessing](#preprocessing)\n",
    "* [Predictions](#predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Explore data / EDA Part 1 \n",
    "My first step was doing EDA on the data. This included creating a pandas dataframe from the csv files included, and then exploring it descriptively and visually to better understand it. \n",
    "\n",
    "Ultimately this led to a better understanding of which columns to drop before model prototyping, and which to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_values = pd.read_csv('tanzania_training_values.csv')\n",
    "training_labels = pd.read_csv('tanzania_training_labels.csv')\n",
    "df = training_values.merge(training_labels, on='id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing  Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# view null values\n",
    "print(\"There are {} duplicates\".format(df.duplicated().sum()))\n",
    "print(\"\\nSummary of null values:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "plt.figure(figsize=(17,10))\n",
    "sns.heatmap(df.isnull().transpose(), xticklabels = False, cbar = False, cmap = 'tab20c_r')\n",
    "plt.title('Missing Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of missing (null) values in the dataset. `scheme_name` had > 28,000 null values and so was determined at this step to be excluded from further analysis. Several other features had > 3K missing null values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(training_labels.status_group.value_counts(normalize=True))\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.set_style('darkgrid')\n",
    "sns.countplot(df.status_group, alpha=1, palette='winter')\n",
    "plt.title('Waterpoint Status')\n",
    "plt.ylabel('Num of waterpoints')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only about 7% of the labels in the dataset belonged to the 'functional needs repair' group. In order to build a model to better include this class in predictions, the use of resampling is used below (Section 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA Geographical Data\n",
    "\n",
    "EDA of `latitude` and `longitude` as gps coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_lat_long(df):\n",
    "    m = folium.Map(width=550, height=350, location=[df.latitude.median(), df.longitude.median()],zoom_start=3)\n",
    "\n",
    "    functional = df[df.status_group == 'functional']\n",
    "    repair = df[df.status_group == 'functional needs repair']\n",
    "    non_functional = df[df.status_group == 'non functional']\n",
    "\n",
    "    functional_fg = folium.FeatureGroup(name='Functional')\n",
    "    repair_fg = folium.FeatureGroup(name='Functional Needs Repair')\n",
    "    non_functional_fg = folium.FeatureGroup(name='Non Functional')\n",
    "\n",
    "    # functional \n",
    "    for lat, long in zip(functional.latitude, functional.longitude):\n",
    "        loc = [lat,long]\n",
    "        folium.Circle(location=loc, color = 'red', radius=5, opacity=.4, tooltip=f'lat: {lat}; long: {long}').add_to(functional_fg)\n",
    "\n",
    "    # functional needs repair \n",
    "    for lat, long in zip(repair.latitude, repair.longitude):\n",
    "        loc = [lat,long]\n",
    "        folium.Circle(location=loc, color = 'blue', radius=5, opacity=.4, tooltip=f'lat: {lat}; long: {long}').add_to(repair_fg)\n",
    "\n",
    "    # non functional \n",
    "    for lat, long in zip(non_functional.latitude, non_functional.longitude):\n",
    "        loc = [lat,long]\n",
    "        folium.Circle(location=loc, color = 'yellow', radius=5, opacity=.4, tooltip=f'lat: {lat}; long: {long}').add_to(non_functional_fg)\n",
    "\n",
    "    m.add_child(functional_fg)\n",
    "    m.add_child(repair_fg)\n",
    "    m.add_child(non_functional_fg)\n",
    "\n",
    "    # turn on layer control\n",
    "    m.add_child(folium.map.LayerControl())\n",
    "\n",
    "    display(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lat_long(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zooming out revealed that while the majority of waterpoints were in Tanzania, some were recorded with a latitude and longitude that was clearly not. Hovering over the point showed that all of those waterpoints placed in the ocean were located at the same latitude and longitude which made it easy to identify them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of incorrectly placed waterpoints: {len(df[df.longitude == 0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA Numeric Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA on `population`, `amount_tsh`, `gps_height`, `construction_year`\n",
    "\n",
    "* `amount_tsh`: Total static head (amount water available to waterpoint)\n",
    "* `gps_height`: Altitude of the well\n",
    "* `population`: Population around the well\n",
    "* `construction_year` - Year the waterpoint was constructed\n",
    "* `num_private` - not described"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.num_private.unique())\n",
    "print(df.num_private.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `num_private` was not described it is unclear how to include this in the data. There were 65 unique numbers, but it is unclear if they are ordinal, continuous or categorical. Due to this, `num_private` was dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(df[df.construction_year == 0]))\n",
    "print(len(df[df.construction_year != 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.construction_year.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were a signgicant number of missing values in `construction_year` (represented as '0'). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df.amount_tsh.describe())\n",
    "print('\\nmean amount_tsh for top 50% of data: {}'.format(df.sort_values(by='amount_tsh', ascending=False).iloc[:int(len(df)*0.5)]['amount_tsh'].mean()))\n",
    "print(f'\\nPercentage of data greater than the amount_tsh mean: {len(df[df.amount_tsh > df.amount_tsh.mean()])/len(df)*100}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `amount_tsh` had a very large range (0 - ~30,000) in values.\n",
    "* Roughly 70% of the records indicate a value of 0 for `amount_tsh`. Only 180 records above 10,000. \n",
    "\n",
    "Due to the large range visualization wasn't useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View differences amongst the four different status groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# drop records with a year of 0, as these are unknown values\n",
    "df[df.construction_year > 0].groupby('status_group').construction_year.median().reset_index().rename(columns={'construction_year':'construction year median'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.groupby('status_group').population.mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby('status_group').amount_tsh.mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby('status_group').gps_height.median().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance it doesn't appear that there is much of a difference between status groups for `population`, but that there are more significant differences among `amount_tsh`, `gps_height` and `construction_year`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Analysis of numeric variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels need to be mapped to numbers in order to prep them for plotting and further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(x):\n",
    "    if x == 'functional':\n",
    "        return 0\n",
    "    elif x == 'functional needs repair':\n",
    "        return 1\n",
    "    else:\n",
    "        return 2 \n",
    "df['status_group_encoded'] = df.status_group.apply(lambda x: map_labels(x))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df[['population','amount_tsh','gps_height','num_private','status_group_encoded']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `population` and `gps_height` don't seem to differ by group, it looks like the highest values for `amount_tsh` are likely to come from functional water wells. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms and Boxplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot histograms and boxplots for each of the three continuous variables to determine if there were any relationships between each and status group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "sns.boxplot(x='amount_tsh', y='status_group', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='population', y='status_group', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.population > 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_continuous(col, df):\n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "    sns.set_style('darkgrid')\n",
    "    sns.histplot(x=col, hue='status_group', data=df, kde=True)\n",
    "    plt.title('{} vs. Status Group'.format(col))\n",
    "    plt.show()\n",
    "    sns.boxplot(x=col, y='status_group', data=df)\n",
    "    plt.title('{} vs. Status Group'.format(col))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_continuous('gps_height', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_continuous('construction_year', df[df.construction_year > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: Distributions between status groups seem to be fairly similar for population and gps height. As seen in the boxplots and above, there does seem to be a material difference in mean for `gps_height` as well as `construction_year`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA Categorical Variables\n",
    "Most of the variables in the dataset were categorical. This involved looking at numeric information regarding the features as well as exploring relationships with status group visually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `amount_tsh` - Total static head (amount water available to waterpoint)\n",
    "* `date_recorded` - The date the row was entered\n",
    "* `funder` - Who funded the well\n",
    "* `gps_height` - Altitude of the well\n",
    "* `installer` - Organization that installed the well\n",
    "* `longitude` - GPS coordinate\n",
    "* `latitude` - GPS coordinate\n",
    "* `wpt_name` - Name of the waterpoint if there is one\n",
    "* `num_private` -\n",
    "* `basin` - Geographic water basin\n",
    "* `subvillage` - Geographic location\n",
    "* `region` - Geographic location\n",
    "* `region_code` - Geographic location (coded)\n",
    "* `district_code` - Geographic location (coded)\n",
    "* `lga` - Geographic location\n",
    "* `ward` - Geographic location\n",
    "* `population` - Population around the well\n",
    "* `public_meeting` - True/False\n",
    "* `recorded_by` - Group entering this row of data\n",
    "* `scheme_management` - Who operates the waterpoint\n",
    "* `scheme_name` - Who operates the waterpoint\n",
    "* `permit` - If the waterpoint is permitted\n",
    "* `construction_year` - Year the waterpoint was constructed\n",
    "* `extraction_type` - The kind of extraction the waterpoint uses\n",
    "* `extraction_type_group` - The kind of extraction the waterpoint uses\n",
    "* `extraction_type_class` - The kind of extraction the waterpoint uses\n",
    "* `management` - How the waterpoint is managed\n",
    "* `management_group` - How the waterpoint is managed\n",
    "* `payment` - What the water costs\n",
    "* `payment_type` - What the water costs\n",
    "* `water_quality` - The quality of the water\n",
    "* `quality_group` - The quality of the water\n",
    "* `quantity` - The quantity of water\n",
    "* `quantity_group` - The quantity of water\n",
    "* `source` - The source of the water\n",
    "* `source_type` - The source of the water\n",
    "* `source_class` - The source of the water\n",
    "* `waterpoint_type` - The kind of waterpoint\n",
    "* `waterpoint_type_group` - The kind of waterpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual EDA\n",
    "During this section, categorical variables were inspected visually. First, they were grouped together by type. Many groupings reference the same general information (ie: `waterpoint_type` and `waterpoint_type_group`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# only group the features with less than 50 unique values \n",
    "pd.DataFrame(df.drop(['amount_tsh','num_private','latitude','longitude','id','gps_height','population','date_recorded'],axis=1).nunique()).sort_values(by=0,ascending=False)[0] < 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# group categorical variables together\n",
    "location = ['basin','region','region_code','district_code']\n",
    "others = ['public_meeting', 'permit']\n",
    "who = ['scheme_management']\n",
    "extraction = ['extraction_type','extraction_type_group','extraction_type_class']\n",
    "management = ['management','management_group']\n",
    "payment = ['payment','payment_type']\n",
    "quality = ['water_quality','quality_group']\n",
    "quantity = ['quantity','quantity_group']\n",
    "source = ['source','source_type','source_class']\n",
    "waterpoint_type = ['waterpoint_type','waterpoint_type_group']\n",
    "cat_vars = location + others + who + extraction + management + payment + quality + quantity + source + waterpoint_type\n",
    "len(cat_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot each feature into a set of subplots \n",
    "Each subplot answers the question: \n",
    "For each status group, how many of each category is represented in the data? \n",
    "\n",
    "Determine if there is a relationship between the feature and status group \n",
    "for each plot, every category annotated with % representation of that category\n",
    "\"\"\"\n",
    "def count_plot_by_group(col):\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    temp = sns.catplot(x=col, kind='count', col='status_group', data=df, height=5, palette='winter')\n",
    "    temp.set_xticklabels(rotation = 45)\n",
    "\n",
    "    for current_plot in range(df.status_group.nunique()):\n",
    "        ax = temp.facet_axis(0,current_plot)\n",
    "        for p in ax.patches:\n",
    "            group = ax.title.get_text().split(\" = \")[1]\n",
    "            total = len(df[df.status_group == group])\n",
    "            if np.isnan(p.get_height()):\n",
    "                height = 0\n",
    "            else:\n",
    "                height = p.get_height() \n",
    "            ax.text(p.get_x()+.015, \n",
    "                    height*1.02, \n",
    "                    '{:0.0f}%'.format(height / total * 100), \n",
    "                    color='black', \n",
    "                    rotation='horizontal',size='small')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in location:\n",
    "    count_plot_by_group(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** The distributions for `district_code` look to be most similar, meaning no significant relationship between `district_code` and `status_group` seems present in the histograms. There does seem to a difference for `basin`, `region` and `region_code`. \n",
    "* Drop: `region_code` (too many unique values), `district_code` (no differences observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regions with the most non functional waterpoints \n",
    "nf_region_count = df[df.status_group == 'non functional'].groupby('region').id.count().reset_index().sort_values(by='id', ascending=False).rename(columns={'id':'count'}).head()\n",
    "nf_region_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in extraction:\n",
    "    count_plot_by_group(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some differences between status groups for the three above. More significant differences observed for `extraction_type_class`, notably under both 'gravity' and 'other' categories. \n",
    "* Drop: `extraction_type_group` (redundant), `extraction_type_class` (redundant) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in management:\n",
    "    count_plot_by_group(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No relationship observed for `management_group`. There does appear to be a slight difference between status groups for `management`. \n",
    "* Dropped: `management_group` (no differences observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in payment:\n",
    "    count_plot_by_group(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some differences noticed in the distributions for `payment_type` and `payment`. Distributions look identical. 1 of the columns can be dropped.  \n",
    "* Drop: `payment`(redundant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in quality:\n",
    "    count_plot_by_group(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some differences observed between differernt status groups. Distributions for `water_quality` and `quality_group` look very similar. More dimensions for `water_quality`\n",
    "* Dropped: `quality_group` (redundant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in quantity:\n",
    "    count_plot_by_group(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be some relationship between `quantity`/`quantity_group` and `status_group`: namely, dry wells are more likely to be in the non functional group. Distributions look identical - 1 of the columns can be dropped.\n",
    "* Drop: `quantity` (redundant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in source:\n",
    "    count_plot_by_group(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in who:\n",
    "    count_plot_by_group(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No relationship oberserved for `scheme_management`. Distributions seem to be roughly similar among status groups. Given this, and number of missing values, concluded this feature can be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in waterpoint_type:\n",
    "    count_plot_by_group(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some differences between status groups for the above and more significant differences observed for `waterpoint_type_group`, notably under both 'communical standpipe' and 'other' categories. \n",
    "\n",
    "In summary, the following columns were dropped based on this EDA: \n",
    "`management_group`,`region_code`,\n",
    "`district_code`,`scheme_management`,`extraction_type_class`,`extraction_type_group`,\n",
    "`payment`,`quantity`,`source_class`,`source`,`quality_group`,`waterpoint_type_group`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boolean Variables\n",
    "* `permit`\n",
    "* `public_meeting`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in others:\n",
    "    ax = sns.catplot(x=col, kind='count', hue='status_group', data=df, height=5, palette='winter')\n",
    "    ax.set_xticklabels(rotation = 30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does appear to be a difference in distribution for `public_meeting` - namely that when `public_meeting` is false, most waterpoints are 'non functional'. Some differences within `permit` observed. When false, there appear to be a more equal number of non functional and functional waterpoints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Others\n",
    "* `funder`\n",
    "* `installer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.installer.isna().sum())\n",
    "print(df.funder.isna().sum())\n",
    "print(len(df[df.installer == '0']))\n",
    "print(len(df[df.funder == '0']))\n",
    "\n",
    "df_2 = df.copy()\n",
    "df_2 = df_2.fillna(value={'funder':'unknown', 'installer':'unknown'})\n",
    "df_2.funder = df_2.funder.apply(lambda x: 'unknown' if x == '0' else x)\n",
    "df_2.installer = df_2.installer.apply(lambda x: 'unknown' if x == '0' else x)\n",
    "df_2.installer.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only include installer with 1000 or more waterpoints \n",
    "installer_count = df_2.groupby('installer').id.count().reset_index().sort_values(by='id', ascending=False).iloc[:20,:].rename(columns={'id':'count'})\n",
    "installer_count = installer_count[installer_count['count'] >= 1000]\n",
    "installer_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# only include funders with 1000 or more waterpoints \n",
    "funder_count = df_2.groupby('funder').id.count().reset_index().sort_values(by='id', ascending=False).iloc[:20,:].rename(columns={'id':'count'})\n",
    "funder_count = funder_count[funder_count['count'] >= 1000]\n",
    "funder_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16,10))\n",
    "sns.set_style('darkgrid')\n",
    "for installer in installer_count.installer:\n",
    "    sns.countplot(df_2[df_2.installer == installer].status_group, palette='winter')\n",
    "    plt.title(installer)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.installer=='RWE'].status_group.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16,10))\n",
    "for funder in funder_count.funder:\n",
    "    sns.countplot(df_2[df_2.funder == funder].status_group, palette = 'winter')\n",
    "    plt.title(funder)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "* When looking at `installer` **RWE**, **Commu** and **DANIDA** all appear to have a different distribution for `status_group` than the overall dataset. \n",
    "* For `funder`, **Government of Tanzania**, **Hesawa**, **Rwssp**, **World Bank** and **Unicef** appear to have a different distribution for `status_group` than the overall dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation / Preprocessing \n",
    "During this step, using insights unlocked from EDA, I cleaned and preprocessed the data to get it ready for use in models. This included: \n",
    "* replacing null values\n",
    "* feature engineering \n",
    "* dropping columns\n",
    "* splitting the data for model training and testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values and Outliers\n",
    "\n",
    "In this section, I explore adding in missing values for two boolean variables `permit` and `public_meeting` as well as removing outliers from the `latitude` / `longitude` columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables: `permit` and `public_meeting`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the number of missing values for each column\n",
    "df[['permit','public_meeting']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the proportion of True and False for each variable \n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "sns.countplot(df.public_meeting,  palette='winter')\n",
    "plt.show()\n",
    "print(df.public_meeting.value_counts(normalize=True))\n",
    "\n",
    "sns.countplot(df.permit,  palette='winter')\n",
    "plt.show()\n",
    "print(df.permit.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the null values for permit \n",
    "isnull = df.permit.isnull()\n",
    "sample = df.permit.dropna().sample(isnull.sum(), replace=True, random_state=123).values\n",
    "df.loc[isnull,'permit'] = sample\n",
    "\n",
    "# replace the null values for public_meeting \n",
    "isnull = df.public_meeting.isnull()\n",
    "sample = df.public_meeting.dropna().sample(isnull.sum(), replace=True, random_state=123).values\n",
    "df.loc[isnull,'public_meeting'] = sample\n",
    "\n",
    "# check to see if there are any null values remaining \n",
    "df[['permit','public_meeting']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize the results again \n",
    "print(df.public_meeting.value_counts(normalize=True))\n",
    "print(df.permit.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables: `latitude` and `longitude`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the map visualization above, it was clear that I needed to move or remove the waterpoints which were not located in Tanzania. Because there were a significant amount of waterpoints which were incorrectly placed (~1800) I decided not to drop those records, but to instead place them at the median latitude and longitude for those groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for status in list(df.status_group.unique()):\n",
    "    lat = df[(df.status_group == status) & (df.longitude != 0)].latitude.median()\n",
    "    long = df[(df.status_group == status) & (df.longitude != 0)].longitude.median()\n",
    "    df['latitude'] = np.where((df.status_group == status) & (df.longitude==0), lat, df.latitude)\n",
    "    df['longitude'] = np.where((df.status_group == status) & (df.longitude==0), long, df.longitude)\n",
    "    \n",
    "plot_lat_long(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "In this section, I explore the variables to prepare for feature engineering. Methods to add new columns are done in the 'define methods' section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable: `construction_year` \n",
    "\n",
    "`construction_year` included a high number of missing values, labeled as '0' in the data. There seemed to be average differences in when a waterpoint was constructed based on status group. `construction_year` was broken into 4 categories: \n",
    "* unknown: `construction_year` = 0 \n",
    "* old: `construction_year` > 0 <= 1994\n",
    "* mid: `construction_year` > 1994 < 2003 \n",
    "* new: `construction_year` >= 2003 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df.construction_year > 0].groupby('status_group').construction_year.median().reset_index().rename(columns={'construction_year':'construction_year_median'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables: `latitude` and `longitude`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the outliers were moved - rather than use the raw latitude and longitude, I decided to use KMeans clustering to group the waterpoints into different areas. Because we don't know what those 'clusters' are beforehand, an unsupervised learning technique was required herer. \n",
    "\n",
    "I used the elbow method to first validate the number of clusters. For each cluster, SSE was calculated. As the number of clusters increase, error decreases but improvements will decline at a certain optimal point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# map the lat and long to x and y coordinates \n",
    "K_clusters = range(1,10)\n",
    "kmeans = [KMeans(n_clusters=i) for i in K_clusters]\n",
    "X = df[['latitude','longitude']]\n",
    "score = [kmeans[i].fit(X).score(X) for i in range(len(kmeans))]\n",
    "\n",
    "# Visualize\n",
    "plt.plot(K_clusters, score)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Elbow Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score levels off after 3.5/4, indicating that there will be minimal benefit from going above 4 clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 4, init ='k-means++') # use 4 clusters from above \n",
    "# fit to calculate clustering \n",
    "kmeans.fit(df[['latitude','longitude']]) \n",
    "centers = kmeans.cluster_centers_ # coord of cluster centers for plotting \n",
    "centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables: `funder` and `installer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use values from EDA above, installers and funders which have a relationship with status_group \n",
    "# create new boollean columns: true if installers and funders from these lists  \n",
    "installers = ['RWE','Commu','DANIDA']\n",
    "funders = ['Government Of Tanzania', 'Hesawa', 'Rwssp', 'World Bank', 'Unicef']\n",
    "\n",
    "df['installer_bool'] = df.installer.apply(lambda x: True if x in installers else False)\n",
    "df['funder_bool'] = df.funder.apply(lambda x: True if x in funders else False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data prepared for model training and fitting using the techniques and analysis from Step 2. To summarize again here briefly: \n",
    "\n",
    "#### Dropped columns\n",
    "26 columns were dropped. Some were irrelevant to `status_group` such as `id`, and others had too many unique values to be encoded (ie: `wpt_name`). \n",
    "\n",
    "#### Engineered Features\n",
    "Three features were engineered: \n",
    "* construction_year_label \n",
    "* cluster_label \n",
    "* installer_bool\n",
    "* funder_bool\n",
    "\n",
    "After this, the data is split into training and test sets, and the categorical variables are one hot encoded so they are ready for model training and fitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define methods <a id='preprocessing'></a>\n",
    "\n",
    "* Take the EDA and feature engineering work from above and encapsulate in methods for reproducability, and organziation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input: values and labels \n",
    "output: 2 dataframes, X and y \n",
    "outliers removed \n",
    "\"\"\"\n",
    "def prep_data(values, labels):\n",
    "    df = values.merge(labels, on='id') # merge the data and labels  \n",
    "    # print('Original columns: {}'.format(df.columns))\n",
    "  \n",
    "    # latitude and longitude - remove outliers (waterpoints located at 0 longitude in the ocean)\n",
    "    for status in list(df.status_group.unique()):\n",
    "        lat_median = df[df.longitude != 0].latitude.median()\n",
    "        long_median = df[df.longitude != 0].longitude.median()\n",
    "        df['latitude'] = np.where((df.longitude==0), lat_median, df.latitude)\n",
    "        df['longitude'] = np.where((df.longitude==0), long_median, df.longitude)\n",
    "               \n",
    "    # convert cat columns into objects \n",
    "    for col in df: \n",
    "        if df[col].dtype == object:\n",
    "            df[col] = df[col].astype('category')   \n",
    "            \n",
    "    # fill in missing values \n",
    "    # replace the null values for permit \n",
    "    isnull = df.permit.isnull()\n",
    "    sample = df.permit.dropna().sample(isnull.sum(), replace=True, random_state=123).values\n",
    "    df.loc[isnull,'permit'] = sample\n",
    "\n",
    "    # replace the null values for public_meeting \n",
    "    isnull = df.public_meeting.isnull()\n",
    "    sample = df.public_meeting.dropna().sample(isnull.sum(), replace=True, random_state=123).values\n",
    "    df.loc[isnull,'public_meeting'] = sample\n",
    "    \n",
    "    # separate into X, y                 \n",
    "    X = df.drop('status_group', axis=1)\n",
    "    y = df.status_group\n",
    "    \n",
    "    return X,y  \n",
    "\n",
    "\"\"\"\n",
    "input: X with non numeric cols converted to category \n",
    "output: dataframe with columns dropped ready for splitting\n",
    "\"\"\"\n",
    "def engineer_features(df):\n",
    "   \n",
    "    # construction_year \n",
    "    def construction_year_code(x):\n",
    "        if x == 0:\n",
    "            return 'unknown'\n",
    "        elif x <= 1994:\n",
    "            return 'old'\n",
    "        elif x < 2003: \n",
    "            return 'mid'\n",
    "        else: \n",
    "            return 'new'\n",
    "    df['construction_year_label'] = df.construction_year.apply(lambda x: construction_year_code(x))\n",
    "    \n",
    "    # latitude / longitude \n",
    "    # using kmeans create 4 clusters, grouping the waterpoints together    \n",
    "    # cluster column \n",
    "    kmeans = KMeans(n_clusters = 4, init ='k-means++', random_state=123) # use 4 clusters from above \n",
    "    # fit to calculate clustering \n",
    "    kmeans.fit(df[['latitude','longitude']]) \n",
    "    # create new column with cluster labels \n",
    "    df['cluster_label'] = kmeans.fit_predict(df[['latitude','longitude']])    \n",
    "  \n",
    "    # installer and funder \n",
    "    installers = ['RWE','Commu','DANIDA']\n",
    "    funders = ['Government Of Tanzania', 'Hesawa', 'Rwssp', 'World Bank', 'Unicef']\n",
    "\n",
    "    df['installer_bool'] = df.installer.apply(lambda x: True if x in installers else False)\n",
    "    df['funder_bool'] = df.funder.apply(lambda x: True if x in funders else False)\n",
    "    \n",
    "    return df \n",
    "\n",
    "\"\"\"\n",
    "after prepping, label encode the target data into numbers \n",
    "one hot encode categorical columns \n",
    "split the data into training and test sets\n",
    "return split data \n",
    "\"\"\"\n",
    "def encode_split_data(X,y,numeric_cols=[]):  \n",
    "    # assign each status group a number \n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    \n",
    "    # split before applying any preprocessing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)\n",
    "\n",
    "    # if numeric columns exist, separate columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        numeric_cols = numeric_cols\n",
    "        cat_cols = X.drop(numeric_cols,axis=1).columns\n",
    "    else:\n",
    "        cat_cols = X.columns\n",
    "    \n",
    "    # one hot encode the cat columns \n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False) #drop=first \n",
    "    \n",
    "    X_train_ohe = pd.DataFrame(ohe.fit_transform(X_train[cat_cols]), columns=ohe.get_feature_names_out(cat_cols))\n",
    "    X_test_ohe = pd.DataFrame(ohe.transform(X_test[cat_cols]), columns=ohe.get_feature_names(cat_cols))\n",
    "    X_train_ohe.index= X_train.index\n",
    "    X_test_ohe.index= X_test.index\n",
    "\n",
    "    # add continuous data to categorical data if numeric columns exist  \n",
    "    if len(numeric_cols) > 0:\n",
    "        X_train = pd.concat([X_train[numeric_cols], X_train_ohe], axis=1)\n",
    "        X_test = pd.concat([X_test[numeric_cols], X_test_ohe], axis=1)\n",
    "    else:\n",
    "        X_train = X_train_ohe\n",
    "        X_test = X_test_ohe\n",
    "    \n",
    "    # create unsplit X and y for cross_val_score \n",
    "    X_ohe = pd.DataFrame(ohe.fit_transform(X[cat_cols]), columns=ohe.get_feature_names_out(cat_cols))\n",
    "    X_all = pd.concat([X[numeric_cols], X_ohe], axis=1)\n",
    "    y_all = y \n",
    "    \n",
    "    print(f'Number of columns after encoding: {len(X_train.columns)}')\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, X_all, y_all\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start from scratch\n",
    "training_values = pd.read_csv('tanzania_training_values.csv')\n",
    "training_labels = pd.read_csv('tanzania_training_labels.csv')\n",
    "\n",
    "to_drop_numeric = ['id','date_recorded','construction_year','longitude','latitude','num_private']\n",
    "\n",
    "to_drop_cat = ['funder','installer','wpt_name','subvillage','lga','ward','scheme_name','recorded_by',\n",
    "               'management_group','region_code','district_code','scheme_management',\n",
    "               'extraction_type_class','extraction_type_group','payment','quantity',\n",
    "               'source_class','source','quality_group','waterpoint_type_group'] #'permit','public_meeting'\n",
    "\n",
    "cols_to_drop = to_drop_numeric + to_drop_cat\n",
    "print(f'{len(cols_to_drop)} columns were dropped.\\n')\n",
    "\n",
    "X,y = prep_data(training_values, training_labels)\n",
    "X = engineer_features(X) # add new columns \n",
    "X = X.drop(cols_to_drop, axis=1)\n",
    "\n",
    "print(f'Columns to keep:{X.columns}')\n",
    "numeric_cols =['gps_height','amount_tsh','population']\n",
    "\n",
    "# encode and split data \n",
    "X_train, X_test, y_train, y_test, X_all, y_all = encode_split_data(X,y,numeric_cols)\n",
    "\n",
    "# keep track of final models for comparison \n",
    "model_dict = {}\n",
    "\n",
    "# for evaluating model fitting \n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "# map the labels to numeric counterparts for use later \n",
    "le = LabelEncoder().fit(y)\n",
    "status_group_dict = {index:label for index,label in enumerate(le.classes_)}\n",
    "status_group_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Prototyping \n",
    "During this step, I take the split data and actual fit and train various classifier models. I fit and train three types of ML models below: \n",
    "- Logistic Regression\n",
    "- Random Forests\n",
    "- XGBoost \n",
    "\n",
    "For each type of model, I first fit a baseline model, and then I tune hyperparameters to attempt to achieve optimal results.  \n",
    "\n",
    "### A note on metrics \n",
    "Here I define an 'in need' waterpoint as belonging to 'non functional' or 'functional needs repair' groups. \n",
    "\n",
    "In terms of metrics, I am most concerned with overall model performance, and how well each model does on in-need groups.\n",
    "\n",
    "From the perspective of the 'in need' waterpoints, I was looking to minimize **false negatives** (missing an 'in need' waterpoint), which would come at the expense of an increase in **false positives** (saying a waterpoint is 'in need' when it isn't). In other words, I was looking to maximize **recall** scores for the minority classes. I sought to optimize recall for these classes during tuning and through the use of oversampling, which comes at the expense of precision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define methods\n",
    "\n",
    "Below are the methods used throughout the current step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Evaluate results of a classification model. \n",
    "\n",
    "Output:\n",
    "Accuracy scores for train and test data \n",
    "Number of false positives\n",
    "Number of false negatives \n",
    "classifcation report\n",
    "plotted confusion matrix  \n",
    "\"\"\"\n",
    "def display_results(model, X_train, X_test, y_train, y_test):\n",
    "    labels=['functional','functional needs repair', 'non functional']\n",
    "    y_hat_test = model.predict(X_test)\n",
    "    y_hat_train = model.predict(X_train)\n",
    "    \n",
    "    matrix = confusion_matrix(y_test, y_hat_test)\n",
    "    # from perspective of non functional and needs repair  \n",
    "    fp = matrix[0][1] + matrix[0][2] # predicted non functional or needs repair even though functional \n",
    "    fn = matrix[1][0] + matrix[2][0] # predicted functional even though it should be non functional or needs repair \n",
    "    tp_f = matrix[0][0]\n",
    "    tp_nr = matrix[1][1]\n",
    "    tp_nf = matrix[2][2]\n",
    "    \n",
    "    print(f\"\\nTraining Accuracy: {accuracy_score(y_train, y_hat_train) :.2%}\\n\")\n",
    "    print(f\"Testing Accuracy: {accuracy_score(y_test, y_hat_test) :.2%}\\n\")\n",
    "    print(f'False positives: {fp}\\n')\n",
    "    print(f'False negatives: {fn}\\n' )\n",
    "    print(f'Total true positives for minority classes: {tp_nr + tp_nf}\\n')\n",
    "    print(classification_report(y_test, y_hat_test, target_names=labels))\n",
    "    plot_confusion_matrix(model, X_test, y_test, xticks_rotation=45, display_labels=labels, cmap='Blues_r')\n",
    "    plt.grid(False)\n",
    "\n",
    "\"\"\" \n",
    "After evaluating model, add the results to a dictionary so that it can later be compared against other models \n",
    "\"\"\"\n",
    "def add_model_dict(model, name, y_true, y_pred, cv_score):\n",
    "    params = model.get_params()  \n",
    "    labels=['functional','functional needs repair', 'non functional']\n",
    "    report = classification_report(y_test, y_pred, target_names=labels, output_dict=True)\n",
    "    accuracy = report['accuracy']*100\n",
    "    functional_precision = report['functional']['precision']\n",
    "    functional_recall = report['functional']['recall']\n",
    "    repair_precision = report['functional needs repair']['precision']\n",
    "    repair_recall = report['functional needs repair']['recall']\n",
    "    nf_precision = report['non functional']['precision']\n",
    "    nf_recall = report['non functional']['recall']\n",
    "    sum_recall = repair_recall + nf_recall \n",
    "    \n",
    "    matrix = confusion_matrix(y_test, y_pred)\n",
    "    fp = matrix[0][1] + matrix[0][2] # predicted non functional or needs repair even though functional \n",
    "    # optimize against \n",
    "    fn = matrix[1][0] + matrix[2][0] # predicted functional even though it should be non functional or needs repair \n",
    "    tp_f = matrix[0][0]\n",
    "    tp_nr = matrix[1][1]\n",
    "    tp_nf = matrix[2][2]\n",
    "    \n",
    "    if name in model_dict.keys():\n",
    "        print('model already found in dictionary.')\n",
    "        return \n",
    "    else:\n",
    "        model_dict[name] = dict(model=model,\n",
    "                                parms=params,\n",
    "                                overall_accuracy=accuracy,\n",
    "                                fn=fn,\n",
    "                                cv_score=cv_score,\n",
    "                                functional_precision=functional_precision,\n",
    "                                functional_recall=functional_recall,\n",
    "                                needs_repair_precision=repair_precision,\n",
    "                                needs_repair_recall=repair_recall,\n",
    "                                nf_precision=nf_precision,\n",
    "                                nf_recall=nf_recall,\n",
    "                                sum_recall=sum_recall)\n",
    "\n",
    "        # return a dataframe with updated information \n",
    "        print('model added to dictionary.')\n",
    "        return \n",
    "\n",
    "def reset_model_dict():\n",
    "    model_dict = {}\n",
    "\n",
    "\"\"\"\n",
    "for a given model, return the time it takes to fit the model and make predictions \n",
    "\"\"\"\n",
    "def training_time(model):\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    stop = time.time()\n",
    "    train_time = round((stop-start),2)    \n",
    "    return train_time \n",
    "\n",
    "\"\"\"plot feature importances for RF and XGBoost models\"\"\" \n",
    "def plot_feature_importances(model, num_features=10):\n",
    "    feature_df = pd.DataFrame(list(zip(xgb_final['classifier'].feature_importances_, X_train.columns.values)),columns=['coef','feature']).sort_values(by='coef', ascending=False)\n",
    "    feature_df = feature_df.iloc[:num_features,:]\n",
    "    n_features = len(feature_df)\n",
    "    sns.set_style('darkgrid')\n",
    "    plt.figure(figsize=(12,8))\n",
    "    sns.barplot(x=feature_df['coef'], y=list(range(n_features)),orient='h', palette='winter') \n",
    "    plt.yticks(np.arange(n_features), feature_df['feature']) \n",
    "    plt.xlabel('Feature importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.show()\n",
    "\n",
    "\"\"\"After a grid search or randomized search, look through a top list of models, \n",
    "and further assess based on effectiveness in minimizing false negatives for in need waterpoints\n",
    "(higher recall scores for the minority classes)\n",
    "Rerank models \"\"\"\n",
    "def get_best_clf(df, classifier):\n",
    "    \n",
    "    # set up lists for dataframe \n",
    "    models=[]\n",
    "    recall_scores =[]\n",
    "    cv_scores=[]\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        params = row['params']\n",
    "        param_dict={}\n",
    "        for k, v in params.items():\n",
    "            new_k = k.split('classifier__')[1]\n",
    "            param_dict[new_k] = v\n",
    "\n",
    "        clf = classifier \n",
    "#         print(param_dict)\n",
    "        clf.set_params(**param_dict)\n",
    "        pipe = imbpipeline(steps=[['smote',SMOTE(random_state=42)],\n",
    "                                  ['classifier',clf]]).fit(X_train, y_train)\n",
    "\n",
    "        y_hat_test = pipe.predict(X_test)\n",
    "        testing_accuracy = accuracy_score(y_test, y_hat_test)\n",
    "\n",
    "        # get the sum of the recall scores for the minority classes \n",
    "        labels=['functional','functional needs repair', 'non functional']\n",
    "        report = classification_report(y_test, y_hat_test, target_names=labels,output_dict=True)\n",
    "        sum_recall = report['functional needs repair']['recall'] + report['non functional']['recall']\n",
    "\n",
    "        # get the cross validation score using all data, not just train data\n",
    "        cv_score = cross_val_score(pipe, X_all, y_all, cv=kf)\n",
    "        cv_score = np.mean(cv_score)\n",
    "\n",
    "        models.append(pipe)\n",
    "        recall_scores.append(sum_recall)\n",
    "        cv_scores.append(cv_score)\n",
    "\n",
    "    new_df = pd.DataFrame(list(zip(models,recall_scores,cv_scores)), columns=['model','recall','cv_score'])\n",
    "    new_df = new_df.sort_values(by=['recall','cv_score'],ascending=False)\n",
    "    top_model = new_df.sort_values(by=['recall','cv_score'],ascending=False).iloc[0].model\n",
    "    return new_df, top_model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models from files \n",
    "\n",
    "Below are the saved models for loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fitted GridSearch objects from files \n",
    "\n",
    "# LOGISTIC REGRESSION\n",
    "with open(\"models/lr_baseline.pickle\", 'rb') as file:\n",
    "    lr_baseline = pickle.load(file)\n",
    "    \n",
    "with open(\"models/lr_baseline_cv_score.pickle\", 'rb') as file:\n",
    "    lr_baseline_cv_score = pickle.load(file)\n",
    "    \n",
    "with open(\"models/lr_grid_smote.pickle\", 'rb') as file:\n",
    "    lr_grid_smote = pickle.load(file)\n",
    "    \n",
    "with open(\"models/lr_grid_no_smote.pickle\", 'rb') as file:\n",
    "    lr_grid_no_smote = pickle.load(file)\n",
    "\n",
    "    \n",
    "# RANDOM FORESTS\n",
    "with open(\"models/rf_baseline_model.pickle\", 'rb') as file:\n",
    "    rf_baseline = pickle.load(file)\n",
    "    \n",
    "with open(\"models/rf_baseline_cv_score.pickle\", 'rb') as file:\n",
    "    rf_baseline_cv_score = pickle.load(file)\n",
    "\n",
    "with open(\"models/rf_random_grid.pickle\",'rb') as file:\n",
    "    rf_random_grid = pickle.load(file)\n",
    "    \n",
    "with open(\"models/rf_grid_smote.pickle\", 'rb') as file:\n",
    "    rf_grid_smote = pickle.load(file)\n",
    "    \n",
    "with open(\"models/best_rf.pickle\", 'rb') as file:\n",
    "    best_rf = pickle.load(file)\n",
    "    \n",
    "    \n",
    "# XGBoost \n",
    "with open(\"models/xgb_baseline.pickle\",'rb') as file:\n",
    "    xgb_baseline = pickle.load(file)\n",
    "                              \n",
    "with open(\"models/xgb_baseline_cv_score.pickle\",'rb') as file:\n",
    "    xgb_baseline_cv_score = pickle.load(file)  \n",
    "    \n",
    "# Load fitted object from files \n",
    "with open(\"models/xgb_random_grid.pickle\",'rb') as file:\n",
    "    xgb_random_grid = pickle.load(file)\n",
    "    \n",
    "# load the fitted objects from files \n",
    "with open(\"models/xgb_grid_smote.pickle\",'rb') as file:\n",
    "    xgb_grid_smote = pickle.load(file)\n",
    "    \n",
    "with open(\"models/best_xgb.pickle\",'rb') as file:\n",
    "    best_xgb = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Models:\n",
    "1. Baseline Model\n",
    "2. Hyperparameter Tuning with GridSearchCV \n",
    "3. Hyperparameter Tuning with GridSearchCV and SMOTE \n",
    "\n",
    "#### Model 1: Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Using **pickle**, I stored the fitted models in files so that they can be easily loaded when running the notebook. Model training can be lengthy, especially when conducting a randomized search or grid search.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_baseline = LogisticRegression(random_state=42, max_iter=2000, multi_class=\"multinomial\")\n",
    "lr_baseline.fit(X_train, y_train)\n",
    "\n",
    "# load from files \n",
    "lr_baseline_cv_score = np.mean(cross_val_score(lr_baseline, X_all, y_all, cv=kf))\n",
    "print(f'Mean Cross Validation Score for an Multinomial Logisitc Regression Model (No Tuning): {lr_baseline_cv_score: .2%}')\n",
    "\n",
    "with open('models/lr_baseline.pickle', 'wb') as f:\n",
    "    pickle.dump(lr_baseline, f)\n",
    "\n",
    "with open('models/lr_baseline_cv_score.pickle', 'wb') as f:\n",
    "    pickle.dump(lr_baseline_cv_score, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = lr_baseline.predict(X_test)\n",
    "add_model_dict(lr_baseline, 'baseline_logreg', y_test, y_pred, lr_baseline_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Baseline Logistic Regression Model')\n",
    "display_results(lr_baseline, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set params for search \n",
    "logreg_grid_params = {'classifier__max_iter':[100,1000,2000],\n",
    "                     'classifier__multi_class':['multinomial'],\n",
    "                     'classifier__C':[1,1e16],\n",
    "                     'classifier__class_weight':[None,'balanced'],\n",
    "                     'classifier__solver':['lbfgs','sag','saga','newton-cg']}\n",
    "\n",
    "# set up pipeline with SMOTE. Do this in pipeline so that smote is applied to every fold \n",
    "pipeline_smote = imbpipeline(steps=[['smote',SMOTE(random_state=42, n_jobs=-1)],\n",
    "                              ['scaler',StandardScaler()],\n",
    "                              ['classifier',LogisticRegression()]])\n",
    "\n",
    "# set up pipeline without SMOTE for comparison. Do this in pipeline so that smote is applied to every fold \n",
    "pipeline = imbpipeline(steps=[['scaler',StandardScaler()],\n",
    "                              ['classifier',LogisticRegression()]])\n",
    "\n",
    "# set up the gridsearch object \n",
    "lr_grid_smote = GridSearchCV(estimator=pipeline_smote, \n",
    "                           param_grid=logreg_grid_params, \n",
    "                           cv=kf, verbose=1)\n",
    "\n",
    "lr_grid_no_smote = GridSearchCV(estimator=pipeline, \n",
    "                           param_grid=logreg_grid_params, \n",
    "                           cv=kf, verbose=1)\n",
    "\n",
    "# fit the grid searches \n",
    "lr_grid_smote = lr_grid_smote.fit(X_train, y_train)\n",
    "lr_grid_no_smote = lr_grid_no_smote.fit(X_train, y_train)\n",
    "\n",
    "# save the models for easy loading on notebook restart \n",
    "with open('models/lr_grid_smote.pickle', 'wb') as f:\n",
    "    pickle.dump(lr_grid_smote, f)\n",
    "    \n",
    "with open('models/lr_grid_no_smote.pickle', 'wb') as f:\n",
    "    pickle.dump(lr_grid_no_smote, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Logistic Regression Model 2 (no SMOTE)')\n",
    "display_results(lr_grid_no_smote.best_estimator_, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Logistic Regression Model 2 (with SMOTE)')\n",
    "display_results(lr_grid_smote.best_estimator_, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('CV Score: {}'.format(lr_grid_smote.best_score_))\n",
    "print('\\nBest params: {}'.format(lr_grid_smote.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "The solvers were different for the baseline model compared with the top results from the grid searches ('lbfgs' for the baseline model versus 'newton-cg' and 'saga'). All models favored a lower C value indicating stronger regularization led to better outcomes here. \n",
    "\n",
    "Using oversampling led to better recall results for the minority classes, most significantly for the 'functional needs repair' category which is least represented in the labels data. Recall for the 'functional needs repair' category increased from .05 to .61. \n",
    "\n",
    "However, due to oversampling, overall model accuracy went down from 72.81% (Model 1) to 63.55% (Model 2). Precision suffered as well. \n",
    "\n",
    "In this case, because we are optimizing for higher recall scores (against false negatives) in the minority classes, the second model was stronger.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrap up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add the final model to the dictionary for comparison later \n",
    "logreg_final = lr_grid_smote.best_estimator_\n",
    "logreg_final_cv = lr_grid_smote.best_score_\n",
    "y_pred = logreg_final.predict(X_test)\n",
    "add_model_dict(logreg_final, 'logreg_final', y_test, y_pred, logreg_final_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests \n",
    "Models:\n",
    "1. Baseline Model\n",
    "2. Hyperparameter Tuning with RandomizedSearchCV (with SMOTE)\n",
    "2. Hyperparameter Tuning with GridSearchCV (with SMOTE)\n",
    "\n",
    "#### Model 1: Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rf_baseline = RandomForestClassifier()\n",
    "rf_baseline.fit(X_train, y_train)\n",
    "rf_baseline_cv_score = np.mean(cross_val_score(rf_baseline, X_all, y_all, cv=kf))\n",
    "print(f'Mean Cross Validation Score for a Random Forest Classifier (No Tuning): {rf_baseline_cv_score: .2%}')\n",
    "\n",
    "with open('models/rf_baseline_model.pickle', 'wb') as f:\n",
    "    pickle.dump(rf_baseline, f)\n",
    "    \n",
    "with open('models/rf_baseline_cv_score.pickle', 'wb') as f:\n",
    "    pickle.dump(rf_baseline_cv_score, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = rf_baseline.predict(X_test)\n",
    "add_model_dict(rf_baseline, 'rf_baseline', y_test, y_pred, rf_baseline_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Baseline Random Forests Model')\n",
    "display_results(rf_baseline, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "\n",
    "Testing accuracy (76.41%) is an improvement over the baseline logistic regression model (70.61%). Large difference in training and testing accuracy indicates the model may be overfitting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: RandomizedSearchCV\n",
    "\n",
    "Due to the fact that this is a large dataset that will incur longer fitting times, a randomized search was used to first refine hyperparameter ranges before doing a gridsearch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# randomized grid search 1 \n",
    "# use SMOTE \n",
    "start = time.time()\n",
    "\n",
    "# define the hyperparameter ranges \n",
    "estimators = np.arange(100,700,50)\n",
    "max_depth = np.arange(10,110,10)\n",
    "criterion = ['gini','entropy']\n",
    "min_samples_leaf = np.arange(1,5,1)\n",
    "min_samples_split = np.arange(1,10,1)\n",
    "random_grid = dict(classifier__n_estimators=estimators, \n",
    "                   classifier__max_depth = max_depth, \n",
    "                   classifier__min_samples_leaf = min_samples_leaf, \n",
    "                   classifier__min_samples_split = min_samples_split, \n",
    "                   classifier__criterion=criterion)\n",
    "\n",
    "pipeline_smote = imbpipeline(steps=[['smote',SMOTE(random_state=42)],\n",
    "                              ['classifier',RandomForestClassifier()]])\n",
    "# set up the object and fit \n",
    "rf_random_grid = RandomizedSearchCV(estimator=pipeline_smote, \n",
    "                                    param_distributions = random_grid, \n",
    "                                    n_iter = 100, cv=kf, random_state=123)\n",
    "\n",
    "rf_random_grid = rf_random_grid.fit(X_train, y_train)\n",
    "\n",
    "stop = time.time()\n",
    "print('time it took: {}'.format(round((stop-start),2)/3600))  \n",
    "\n",
    "# save the fitted object as a file for ease of access \n",
    "with open('models/rf_random_grid.pickle','wb') as f:\n",
    "    pickle.dump(rf_random_grid, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Random Forests Model 2 (RandomizedSearchCV)')\n",
    "display_results(rf_random_grid.best_estimator_, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('RF Randomized Search CV Score: {}'.format(rf_random_grid.best_score_))\n",
    "print('RF Randomized Search Best params: {}'.format(rf_random_grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rf_random_grid_results = pd.DataFrame(rf_random_grid.cv_results_)\n",
    "rf_random_grid_results.sort_values(by='rank_test_score', ascending=True).head().iloc[:,4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes/Interpretation:** \n",
    "\n",
    "* The top 5 models from the search had either 650 or 300 estimators. Variance among the other parameters. \n",
    "* Conducting the randomized search incurred signficant time (4.8 hours). \n",
    "* The difference between training and testing scores grew smaller compared to the baseline model produced from the search, indicating less overfitting. \n",
    "* Improvements seen in testing accuracy in the randomized search model (76.17%) versus the baseline random forests model (74.70%). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3: GridSearchCV \n",
    "\n",
    "Using parameters from the randomized search, apply further tuning with a grid search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# set params for search \n",
    "estimators = [275, 300, 325, 350]\n",
    "max_depth = [60,70,80,90]\n",
    "criterion = ['gini','entropy']\n",
    "min_samples_leaf = [2]\n",
    "min_samples_split = [2,3,4,5]\n",
    "\n",
    "param_grid_final = dict(classifier__n_estimators=estimators, \n",
    "                        classifier__max_depth = max_depth,\n",
    "                        classifier__min_samples_leaf = min_samples_leaf,\n",
    "                        classifier__min_samples_split = min_samples_split,\n",
    "                        classifier__criterion=criterion)\n",
    "\n",
    "# set up pipelines, 1 with smote and 1 without. Do this in pipeline so that smote is applied to every fold \n",
    "pipeline_smote = imbpipeline(steps=[['smote',SMOTE(random_state=42)],\n",
    "                              ['classifier',RandomForestClassifier()]])\n",
    "\n",
    "# set up the gridsearch object \n",
    "rf_grid_smote = GridSearchCV(estimator=pipeline_smote,\n",
    "                             param_grid = param_grid_final,\n",
    "                             cv=kf, verbose=1)\n",
    "# fit the grid searches \n",
    "rf_grid_smote = rf_grid_smote.fit(X_train, y_train)\n",
    "\n",
    "stop = time.time()\n",
    "print('time it took: {}'.format(round((stop-start),2)/3600))  \n",
    "\n",
    "# save the models for easy loading on notebook restart \n",
    "with open('models/rf_grid_smote.pickle','wb') as f:\n",
    "    pickle.dump(rf_grid_smote, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Random Forests Model 3 (GridSearchCV/SMOTE)')\n",
    "display_results(rf_grid_smote.best_estimator_.named_steps['classifier'], X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\nCV Score: {}'.format(rf_grid_smote.best_score_))\n",
    "print('\\nBest params: {}'.format(rf_grid_smote.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid_smote_df = pd.DataFrame(rf_grid_smote.cv_results_)\n",
    "rf_grid_smote_df.sort_values(by='rank_test_score', ascending=True).iloc[:,4:]\n",
    "best_rf  = get_best_clf(rf_grid_smote_df.head(10), RandomForestClassifier())\n",
    "\n",
    "with open('models/best_rf.pickle','wb') as f:\n",
    "    pickle.dump(best_rf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_df, best_rf_model = best_rf[0], best_rf[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'Final Random Forests Model CV Score: {best_rf_df.iloc[0].cv_score}')\n",
    "print('Final Random Forests Model')\n",
    "display_results(best_rf_model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "\n",
    "Model 3 (GridSearchCV) had a stronger cross validation score than the baseline model (76% vs 74%). Model 3 had stronger recall scores for the 'functional' and 'functional needs repair' classes indicating it made more overall correct classifications for those categories. Overall testing accuracy was also stronger than for baseline model. \n",
    "\n",
    "Overall, when comparing models fitted with oversampled data, the Random Forests model appears to be a stronger fit than the logistic regression one (.75 vs .62 cross validation score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the model to the dictionary for comparison later \n",
    "rf_final = best_rf_model\n",
    "rf_final_cv = best_rf_df.iloc[0].cv_score\n",
    "y_pred = rf_final.predict(X_test)\n",
    "add_model_dict(rf_final, 'rf_final', y_test, y_pred, rf_final_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost\n",
    "\n",
    "Models:\n",
    "1. Baseline Model\n",
    "2. Hyperparameter Tuning with RandomizedSearchCV (with SMOTE)\n",
    "2. Hyperparameter Tuning with GridSearchCV (with SMOTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit a baseline xgboost classifier model   \n",
    "xgb_baseline = XGBClassifier(eval_metric = 'merror')\n",
    "xgb_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Get baseline cross validation score for XGBoost Classifier \n",
    "xgb_baseline_cv_score = np.mean(cross_val_score(xgb_baseline, X_all, y_all, cv=kf))\n",
    "print(f'Mean Cross Validation Score for an XGBoost Classifier (No Tuning): {xgb_baseline_cv_score: .2%}')\n",
    "\n",
    "with open('models/xgb_baseline.pickle','wb') as f:\n",
    "    pickle.dump(xgb_baseline, f)\n",
    "    \n",
    "with open('models/xgb_baseline_cv_score.pickle','wb') as f:\n",
    "    pickle.dump(xgb_baseline_cv_score, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_results(xgb_baseline, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation: \n",
    "Baseline XGBoost classifier model performs better on overall accuracy than the logisitic regression and random forests baseline models. Minority class recall is still weak. Hyperparameter tuning and oversampling method performed as a next step to address this. \n",
    "\n",
    "* XGBoost baseline cross validation score: **77.54%**\n",
    "* Random Forests cross validation score: **75.63%**\n",
    "* Logistic regression cross validation score: **72.81%**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add baseline model to dictionary \n",
    "y_pred = xgb_baseline.predict(X_test)\n",
    "add_model_dict(xgb_baseline, 'xgb_baseline', y_test, y_pred, xgb_baseline_cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: RandomizedSearchCV \n",
    "\n",
    "A randomized search was used to first refine hyperparameter ranges before doing a gridsearch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "#set params for search \n",
    "random_grid = {'classifier__max_depth':np.arange(5,20,1),\n",
    "               'classifier__min_child_weight':np.arange(1,9,1), # between 0 and 1\n",
    "               'classifier__learning_rate':[.0001,.001,.01,.05,.1,.2,.3,.4],\n",
    "               'classifier__colsample_bylevel':np.arange(0.3,1.1,.1),\n",
    "               'classifier__colsample_bytree':np.arange(.3,1.1,.1),\n",
    "               'classifier__n_estimators':np.arange(100,600,25) # default 100, number of trees, number of boosting rounds \n",
    "              }\n",
    "\n",
    "pipe = imbpipeline(steps=[['smote',SMOTE(random_state=42)],\n",
    "                          ['classifier',XGBClassifier(eval_metric = 'merror')]])\n",
    "\n",
    "\n",
    "# set up the randomizedsearchcv object \n",
    "xgb_random_grid = RandomizedSearchCV(estimator=pipe, \n",
    "                                    param_distributions=random_grid, \n",
    "                                    scoring='accuracy',\n",
    "                                    n_iter=200, \n",
    "                                    cv=3, verbose=1)\n",
    "# fit the object  \n",
    "xgb_random_grid.fit(X_train, y_train)\n",
    "\n",
    "stop = time.time()\n",
    "print('time it took: {} hours.'.format(round((stop-start)/3600,2)))  \n",
    "\n",
    "# with open('models/xgb_random_grid.pickle','wb') as f:\n",
    "#     pickle.dump(xgb_random_grid, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('XGBoost Model 2 (RandomizedSearchCV/SMOTE)')\n",
    "display_results(xgb_random_grid.best_estimator_.named_steps['classifier'], X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = np.mean(cross_val_score(xgb_random_grid.best_estimator_.named_steps['classifier'], X_train, y_train, cv=kf))\n",
    "print(f'Mean Cross Validation Score for an XGBoost Classifier (RandomizedSearchCV): {score: .2%}')\n",
    "print('RF RandomizedSearchCV Best params: {}'.format(xgb_random_grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xgb_random_grid_df = pd.DataFrame(xgb_random_grid.cv_results_).sort_values(by='rank_test_score', ascending=True)\n",
    "xgb_random_grid_df.head().iloc[:,4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3: GridSearchCV \n",
    "\n",
    "Using the parameters established above, a grid search was conducted to further optimize model results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if not xg_grid:\n",
    "# set params for search using the randomized search params as guide\n",
    "\n",
    "start = time.time()\n",
    "  \n",
    "xg_grid_params = {'classifier__n_estimators': np.arange(400,500,25),\n",
    "                  'classifier__max_depth': [16,17,18],\n",
    "                  'classifier__min_child_weight': [6,7,8],\n",
    "                  'classifier__learning_rate':[0.025,0.05], \n",
    "                  'classifier__colsample_bytree': [0.6,0.7], \n",
    "                  'classifier__colsample_bylevel': [0,4,0.5]}\n",
    "\n",
    "# Set up smote in pipeline so that smote is applied to every fold \n",
    "pipe_smote = imbpipeline(steps=[['smote',SMOTE(random_state=42)],\n",
    "                                ['classifier',XGBClassifier(eval_metric = 'merror')]])\n",
    "\n",
    "xgb_grid_smote = GridSearchCV(estimator=pipe_smote,\n",
    "                              param_grid=xg_grid_params,\n",
    "                              scoring='accuracy',\n",
    "                              cv=3,\n",
    "                              verbose=1)\n",
    "\n",
    "# fit the grid searches \n",
    "xgb_grid_smote = xgb_grid_smote.fit(X_train, y_train)\n",
    "\n",
    "stop = time.time()\n",
    "print('time it took: {} hours'.format(round((stop-start),2)/3600)) \n",
    "\n",
    "# # save the models for easy loading on notebook restart \n",
    "# with open(\"models/xgb_grid_smote.pickle\",'wb') as f:\n",
    "#     pickle.dump(xgb_grid_smote, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# with funder and installer engineered features \n",
    "print('XGBoost Model 3 (GridSearchCV/SMOTE)')\n",
    "display_results(xgb_grid_smote.best_estimator_.named_steps['classifier'], X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Step: \n",
    "Take the top 25 models from the GridSearchCV and rank them based on recall scores and cross validation using the entire dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xgb_grid_smote_df = pd.DataFrame(xgb_grid_smote.cv_results_).sort_values(by='rank_test_score', ascending=True)\n",
    "xgb_grid_smote_df.head().iloc[:,4:]\n",
    "best_xgb = get_best_clf(xgb_grid_smote_df.head(25), XGBClassifier(eval_metric='merror'))\n",
    "\n",
    "best_xgb_df, best_xgb_model  = best_xgb[0], best_xgb[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb_model  = best_xgb[1]\n",
    "\n",
    "print(f'Final XGBoost Model CV Score: {best_xgb_df.iloc[0].cv_score}')\n",
    "print('\\nFinal XGBoost Model')\n",
    "display_results(best_xgb_model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "\n",
    "* When compared to the baseline model, the final XGBoost model had a slightly weaker cross validation score (77.63% vs 77.98%) and a slightly weaker testing score (77.96% for Model 3  vs 78.39% for Baseline model). \n",
    "\n",
    "* Model 3 showed significant improvements for recall scores on the minority classes ('functional' and 'functional needs repair') indicating it made more overall correct classifications for those categories. \n",
    "\n",
    "* Overall, when comparing models fitted with oversampled data, the XGBoost model appears to be a stronger fit than both the logistic regression and random forest models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add the models to dictionary \n",
    "xgb_final = best_xgb_model\n",
    "xgb_final_cv = best_xgb_df.cv_score.iloc[0]\n",
    "y_pred = xgb_final.predict(X_test)\n",
    "add_model_dict(xgb_final, 'xgb_final', y_test, y_pred, xgb_final_cv)\n",
    "\n",
    "# with open(\"models/xgb_final.pickle\",'wb') as f:\n",
    "#     pickle.dump(xgb_final, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(zip(xgb_final['classifier'].feature_importances_, X_train.columns.values)),columns=['coef','feature']).sort_values(by='coef', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_feature_importances(xgb_final['classifier'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further analysis on top features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `quantity_group_dry`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the quantity group dry feature by status group \n",
    "sns.set_style('darkgrid')\n",
    "temp = pd.concat([X_all, pd.DataFrame(y_all, columns=['status_group'])], axis=1)\n",
    "sns.catplot(x='quantity_group_dry', kind='count', hue='status_group', \n",
    "            data=temp[temp.quantity_group_dry == 1], height=5, palette='winter', legend=False)\n",
    "plt.legend(title='Status Group',labels=['functional','functional needs repair','non functional'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(status_group_dict)\n",
    "\n",
    "temp[temp.quantity_group_dry == 1].status_group.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `waterpoint_type_other`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the quantity group dry feature by status group \n",
    "sns.set_style('darkgrid')\n",
    "temp = pd.concat([X_all, pd.DataFrame(y_all, columns=['status_group'])], axis=1)\n",
    "sns.catplot(x='waterpoint_type_other', kind='count', hue='status_group', \n",
    "            data=temp[temp.waterpoint_type_other == 1], height=5, palette='winter', legend=False)\n",
    "plt.legend(title='Status Group',labels=['functional','functional needs repair','non functional'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[temp.waterpoint_type_other == 1].status_group.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Analysis\n",
    "During this step, I look at the data for each of the models chosen from above, alongside their baseline counterparts in order to make a final determination on classification model. The three algorithms used: \n",
    "- Logistic Regression\n",
    "- Random Forests\n",
    "- XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(model_dict,orient='index').reset_index()\n",
    "results_df = results_df.rename(columns={'index':'model_name'})\n",
    "results_df['train_time'] = results_df.model.apply(lambda x: training_time(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.set_style('darkgrid')\n",
    "ax = sns.barplot(x='model_name', y='nf_recall',data=results_df.sort_values(by='nf_recall',ascending=False), palette='winter')\n",
    "ax.set_title('Model Performance on the \"Non Functional\" Class')\n",
    "ax.set_ylabel('Recall Score (Non Functional)')\n",
    "ax.set_xlabel('Model Type')\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation = 30)\n",
    "ax.set_ylim(0.5,0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.set_style('darkgrid')\n",
    "ax = sns.barplot(x='model_name', y='needs_repair_recall',data=results_df.sort_values(by='needs_repair_recall',ascending=False), palette='winter')\n",
    "ax.set_title('Model Performance on the \"Needs Repair\" Class')\n",
    "ax.set_ylabel('Recall Score (Needs Repair)')\n",
    "ax.set_xlabel('Model Type')\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation = 30)\n",
    "ax.set_ylim(0,0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see the effects of the SMOTE oversampling technique on each of the baseline models. An increase in recall here corresponds to finding more of the minority class ('functional needs repair'). The model say 'yes' more frequently to this class, at the expense of more false positives (incorrectly classifying a waterwell as this class). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to using oversampling, the precision doesn't go up with the final model. Recall and precision are inversely related and the current graph is almost an opposite to the graph above. \n",
    "\n",
    "This corresponds to the fact that there are more false positives. The model is more likely to 'guess' that a waterwell is from the 'functional needs repair' class than without oversampling, but those guesses are also incorrect. \n",
    "\n",
    "The final xgboost model precision goes up, and this may be due to effective hyperparameter tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_df.sort_values(by='overall_accuracy', ascending=False)[['model_name','cv_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the final logistic regression model had the best recall score for our minority class, it scored very low on overall accuracy, which is one reason it wasn't chosen as the strongest model. XGBoost outperformed other models on overall testing accuracy, as well as on the minority class 'functional needs repair' and the number of false negatives (missing waterpoints that need to be repaired/replaced). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison (Radar Chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_df[results_df.model_name.apply(lambda x: 'baseline' not in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# normalize the columns for the radar chart \n",
    "results_df['train_time_normalized_inverse'] = 1- ((results_df['train_time'] - np.min(results_df.train_time))/\n",
    "                                          (np.max(results_df.train_time) - np.min(results_df.train_time)))\n",
    "\n",
    "results_df['cv_score_normalized'] = ((results_df['cv_score'] - np.min(results_df.cv_score))/\n",
    "                                        (np.max(results_df.cv_score) - np.min(results_df.cv_score)))\n",
    "\n",
    "results_df['needs_repair_recall_normalized'] = ((results_df['needs_repair_recall'] - np.min(results_df.needs_repair_recall))/\n",
    "                                          (np.max(results_df.needs_repair_recall) - np.min(results_df.needs_repair_recall)))\n",
    "\n",
    "results_df['nf_recall_normalized'] = ((results_df['nf_recall'] - np.min(results_df.nf_recall))/\n",
    "                                          (np.max(results_df.nf_recall) - np.min(results_df.nf_recall)))\n",
    "\n",
    "results_df['nf_precision_normalized'] = ((results_df['nf_precision'] - np.min(results_df.nf_precision))/\n",
    "                                          (np.max(results_df.nf_precision) - np.min(results_df.nf_precision)))\n",
    "\n",
    "results_df['needs_repair_precision_normalized'] = ((results_df['needs_repair_precision'] - np.min(results_df.needs_repair_precision))/\n",
    "                                                    (np.max(results_df.needs_repair_precision) - np.min(results_df.needs_repair_precision)))\n",
    "\n",
    "# drop the baseline models \n",
    "results_df_plot = results_df[results_df.model_name.apply(lambda x: 'baseline' not in x)]\n",
    "\n",
    "print(list(results_df_plot.model_name))\n",
    "\n",
    "# create the radar chart \n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "\n",
    "radar_df = results_df_plot[['cv_score_normalized',\n",
    "                            'needs_repair_recall_normalized',\n",
    "                            'nf_recall_normalized',\n",
    "                            'needs_repair_precision_normalized',\n",
    "                            'nf_precision_normalized',\n",
    "                            'train_time_normalized_inverse']]\n",
    "\n",
    "model_names = list(results_df_plot.model_name) #list(results_df_plot.model_name)\n",
    "\n",
    "categories = ['Cross Validation Score',\n",
    "              'Recall (Needs Repair)', \n",
    "              'Recall (Non Functional)', \n",
    "              'Precision (Functional Needs Repair)',\n",
    "              'Precision (Non Functional)',\n",
    "              'Train Time (Inverse)']\n",
    "\n",
    "categories = [*categories, categories[0]]\n",
    "\n",
    "models=[]\n",
    "for i in range(len(model_names)):\n",
    "    temp = list(radar_df.iloc[i].values)\n",
    "    temp = [*temp, temp[0]]\n",
    "    models.append(temp)\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Scatterpolar(r=models[0], theta=categories, fill='toself', name= model_names[0]),\n",
    "        go.Scatterpolar(r=models[1], theta=categories, fill='toself',  name= model_names[1]),\n",
    "        go.Scatterpolar(r=models[2], theta=categories, fill='toself', name= model_names[2]),\n",
    "#         go.Scatterpolar(r=models[3], theta=categories, name= model_names[3]),\n",
    "#         go.Scatterpolar(r=models[4], theta=categories, name= model_names[4]),\n",
    "#         go.Scatterpolar(r=models[5], theta=categories, name= model_names[5])\n",
    "    ],\n",
    "    layout=go.Layout(\n",
    "        title=go.layout.Title(text='Model Comparison'),\n",
    "        polar={'radialaxis':{'visible':True}},\n",
    "        showlegend=True\n",
    "    )\n",
    ")\n",
    "pyo.plot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Selection \n",
    "[Source](https://towardsdatascience.com/feature-selection-using-python-for-classification-problem-b5f00a1c7028)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the final XGBoost model chosen, I decided to do feature selection with Recursive Feature Elimination to determine whether the model could be optimized even further by using RFE to decide on which features to keep, as opposed to EDA used above. To do so I took the following steps: \n",
    "1. Took the original data, and only dropped the columns with missing values. After one-hot-encoding, this left 344 columns compared to 105 columns originally. \n",
    "2. Conduct an RFE grid search using the RFECV package from sklearn and the XGBoost model with the parameters from above. The output gave me an final list and number of features. \n",
    "3. Visualize the results and compare to the XGBoost model above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/rfecv.pickle','rb') as f:\n",
    "    rfecv = pickle.load(f)\n",
    "    \n",
    "with open(\"models/xgb_final_rfe.pickle\",'rb') as f:\n",
    "    xgb_final_rfe = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start from scratch\n",
    "training_values = pd.read_csv('tanzania_training_values.csv')\n",
    "training_labels = pd.read_csv('tanzania_training_labels.csv')\n",
    "\n",
    "to_drop_numeric = ['id','date_recorded','construction_year','longitude','latitude','amount_tsh']\n",
    "\n",
    "to_drop_cat = ['funder','installer','wpt_name','subvillage','ward','scheme_name','recorded_by', \n",
    "               'scheme_management','permit']\n",
    "\n",
    "cols_to_drop = to_drop_numeric + to_drop_cat\n",
    "print(f'{len(cols_to_drop)} columns were dropped')\n",
    "\n",
    "X,y = prep_data(training_values, training_labels)\n",
    "X = engineer_features(X) # add new columns \n",
    "X = X.drop(cols_to_drop, axis=1)\n",
    "\n",
    "print(f'Columns to keep:{X.columns}')\n",
    "numeric_cols =['gps_height','num_private','population']\n",
    "\n",
    "# encode and split data \n",
    "X_train_fe, X_test_fe, y_train_fe, y_test_fe, X_all_fe, y_all_fe = encode_split_data(X,y,numeric_cols)\n",
    "\n",
    "# keep track of final models for comparison \n",
    "model_dict = {}\n",
    "\n",
    "# for evaluating model fitting \n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "print(len(X_train_fe.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# using the optimal model found in the last step conduct a RFECV \n",
    "xgb_rfe = xgb_grid_smote.best_estimator_.named_steps['classifier']\n",
    "\n",
    "rfecv = RFECV(estimator=xgb_rfe,\n",
    "              step=1,\n",
    "              cv=3,\n",
    "              scoring='accuracy',\n",
    "              min_features_to_select=1, \n",
    "              verbose=0)\n",
    "\n",
    "rfecv.fit(X_train_fe, y_train_fe)\n",
    "\n",
    "print(f'Optimal number of features: {rfecv.n_features_}')\n",
    "\n",
    "# Filter X_train and X_test using the columns selected by RFECV \n",
    "X_train_rfe = X_train_fe.loc[:,rfecv.support_]\n",
    "X_test_rfe = X_test_fe.loc[:,rfecv.support_]\n",
    "\n",
    "# save the columns for later use \n",
    "rfe_cols = X_train_rfe.columns\n",
    "\n",
    "with open(\"models/rfecv.pickle\",'wb') as f:\n",
    "    pickle.dump(rfecv, f)\n",
    "    \n",
    "with open(\"models/rfe_cols.pickle\",'wb') as f:\n",
    "    pickle.dump(rfe_cols, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_features = pd.DataFrame(columns=['feature','support','ranking'])\n",
    "# for i in range(X_train_rfe.shape[1]):\n",
    "#     row = {'feature':i, 'feature_name':X_train_rfe.columns[i], 'support':rfecv.support_[i],'ranking':rfecv.ranking_[i]}\n",
    "#     df_features = df_features.append(row, ignore_index=True)\n",
    "# df_features.sort_values(by='ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1, len(rfecv.grid_scores_)+1), rfecv.grid_scores_)\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_final_rfe = xgb_final.fit(X_train_rfe, y_train_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"models/xgb_final_rfe.pickle\",'wb') as f:\n",
    "#     pickle.dump(xgb_final_rfe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_final_rfe_cv_score = np.mean(cross_val_score(xgb_final_rfe, X_all_fe, y_all_fe, cv=kf))\n",
    "print(f'Mean Cross Validation Score for an XGBoost Classifier (with RFE): {xgb_final_rfe_cv_score: .2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# xgb_rfe = xgb_grid_rfe.best_estimator_.named_steps['classifier']\n",
    "print('XGBoost Model with Recursive Feature Elimination\\n')\n",
    "display_results(xgb_final_rfe, X_train_rfe, X_test_rfe, y_train_fe, y_test_fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considerations \n",
    "* This model does outperform the final model chosen above (78.4% vs. 77.6%), although at the expense of increasing dimensionality (312 vs. 109 features). \n",
    "* Results indicated a stronger model, but the resulting increase in dimensions means longer fitting and prediction times. This should be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Predictions <a id='predictions'></a>\n",
    "Here I define a method for making predictions with the chosen model. This method takes in data with unknown labels, and assigns a label ('functional', 'non functional', 'functional needs repair') to each of the rows in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('tanzania_test_values.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/rfe_cols.pickle\",'rb') as f:\n",
    "    rfe_cols = pickle.load(f)\n",
    "    \n",
    "with open('models/df_final.pickle', 'rb') as f:\n",
    "    df_final = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "take in testing data (as dataframe) about well(s) and make prediction(s) about status \n",
    "return a dataframe, each row containing an id and features for a well and a corresponding prediction (status group)\n",
    "\"\"\"\n",
    "\n",
    "to_drop_numeric = ['id','date_recorded','construction_year','longitude','latitude','amount_tsh']\n",
    "\n",
    "to_drop_cat = ['funder','installer','wpt_name','subvillage','ward','scheme_name','recorded_by', \n",
    "               'scheme_management','permit']\n",
    "\n",
    "cols_to_drop = to_drop_numeric + to_drop_cat \n",
    "\n",
    "# save columns for final dataframe \n",
    "ids = data['id'] \n",
    "funders = data['funder']\n",
    "installers=data['installer']\n",
    "construction_years = data['construction_year']\n",
    " \n",
    "# convert cat columns into objects \n",
    "for col in data: \n",
    "    if data[col].dtype == object:\n",
    "        data[col] = data[col].astype('category')   \n",
    "\n",
    "# REMOVE OUTLIERS \n",
    "# latitude and longitude - remove outliers (waterpoints located at 0 longitude in the ocean)\n",
    "lat = data[data.longitude != 0].latitude.median()\n",
    "long = data[data.longitude != 0].longitude.median()\n",
    "data['latitude'] = np.where((data.longitude==0), lat, data.latitude)\n",
    "data['longitude'] = np.where((data.longitude==0), long, data.longitude)\n",
    "\n",
    "lat_lon = data[['latitude','longitude']]\n",
    "\n",
    "# MISSING VALUES\n",
    "# replace the null values for permit \n",
    "isnull = data.permit.isnull()\n",
    "sample = data.permit.dropna().sample(isnull.sum(), replace=True, random_state=123).values\n",
    "data.loc[isnull,'permit'] = sample\n",
    "\n",
    "# replace the null values for public_meeting \n",
    "isnull = data.public_meeting.isnull()\n",
    "sample = data.public_meeting.dropna().sample(isnull.sum(), replace=True, random_state=123).values\n",
    "data.loc[isnull,'public_meeting'] = sample\n",
    "\n",
    "# FEATURE ENGINEERING \n",
    "data = engineer_features(data) # add new columns \n",
    "\n",
    "# DROP COLUMNS \n",
    "data = data.drop(cols_to_drop, axis=1)\n",
    "\n",
    "# CREATE DATAFRAME \n",
    "numeric_cols =['gps_height','num_private','population']\n",
    "cat_cols = data.drop(numeric_cols,axis=1).columns # get cat cols\n",
    "rfe_col_filter = rfe_cols\n",
    "\n",
    "# save copy of dataframe for reading later \n",
    "df_final = pd.concat([data[cat_cols],data[numeric_cols]],axis=1)\n",
    "\n",
    "# for predictions \n",
    "# one hot encode \n",
    "ohe = OneHotEncoder()\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse=False) #drop=first \n",
    "data_ohe = pd.DataFrame(ohe.fit_transform(data[cat_cols]), columns=ohe.get_feature_names(cat_cols))\n",
    "\n",
    "# combine one hot encoded columns and numeric columns \n",
    "# only include columns from RFE \n",
    "data_final = pd.concat([data_ohe, data[numeric_cols]], axis=1)\n",
    "data_final = data_final[rfe_col_filter]\n",
    "\n",
    "# MAKE PREDICTIONS \n",
    "# make preds\n",
    "preds = pd.DataFrame(xgb_final_rfe.predict(data_final), columns=['status_group_enc'])\n",
    "preds['status_group'] = preds.status_group_enc.apply(lambda x: status_group_dict[x])\n",
    "\n",
    "# append predictions to dataframe without encoding \n",
    "df_final = pd.concat([ids, df_final, preds, lat_lon, funders, installers, construction_years],axis=1)\n",
    "df_final.head()\n",
    "\n",
    "# with open('models/df_final.pickle', 'wb') as f:\n",
    "#      pickle.dump(df_final, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regions with the most non functional waterpoints \n",
    "# nf_region_count_final = df_final[df_final.status_group == 'non functional'].groupby('region').id.count().reset_index().sort_values(by='id', ascending=False).rename(columns={'id':'count'}).head()\n",
    "# nf_region_count_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regions with the most non functional or needs repair waterpoints \n",
    "in_need = df_final[(df_final.status_group == 'non functional') | (df_final.status_group == 'functional needs repair')]\n",
    "nf_repair_region_count_final = in_need.groupby('region').id.count().reset_index().sort_values(by='id', ascending=False).rename(columns={'id':'count'}).head()\n",
    "nf_repair_region_count_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regions with the most non functional waterpoitns are also the regions with the most non functional and needs repair waterpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "in_need_region = df_final[df_final.region.apply(lambda x: x  in nf_repair_region_count_final.region.values)]\n",
    "plot_lat_long(in_need_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final[df_final.quantity_group == 'dry'].status_group.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_need_installer = in_need.groupby('installer').id.count().reset_index().sort_values(by='id', ascending=False).rename(columns={'id':'count'}).head()\n",
    "in_need_installer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.countplot(df_final[df_final.installer == 'RWE'].status_group, palette='winter')\n",
    "plt.title('Number of RWE Installed Waterpoints')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # show the number of non functional and needing repair waterpoints, grouped by funder \n",
    "# in_need_funder = in_need.groupby('funder').id.count().reset_index().sort_values(by='id', ascending=False).rename(columns={'id':'count'}).head()\n",
    "# in_need_funder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show the number of non functional and needing repair waterpoints, grouped by year category  \n",
    "# in_need_years = in_need.groupby('construction_year_label').id.count().reset_index().sort_values(by='id', ascending=False).rename(columns={'id':'count'})\n",
    "# in_need_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='construction_year_label', kind='count', col='status_group', data=in_need[in_need.construction_year_label != 'unknown'], height=5, palette='winter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "in_need[(in_need.construction_year_label != 'unknown') & (in_need.status_group == 'non functional')].construction_year_label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "\n",
    "## Next Steps \n",
    "* Other machine learning algorithms: KNN, Naive Bayes, and Support Vector Machines are missing from the above trials. Due to the size of the data, training time should be considered. \n",
    "* More feature selection techniques: Due to the larger number of features present in the dataset, utilizing other methods to refine the feature list could improve model performance and efficiency. \n",
    "* Metrics: As mentioned above, the final models chosen in this analysis were based on optimizing for recall of the minority classes. Other metrics, like f-1 score and precision can be prioritized in future studies. In addition other resampling techniques could be experimented with to see if this positively affects results. \n",
    "* More feature engineering: Conducting more EDA and experimenting with other ways of creating new features could yield more positive outcomes. \n",
    "* Further investigation: Digging deeper into some of the insights. For example, it seems like waterpoints around Lake Victoria have more issues. Why? What makes waterpoints installed by certain parties more likely to have issues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
